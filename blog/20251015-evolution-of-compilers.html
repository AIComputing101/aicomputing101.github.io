<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Evolution of Compilers: From "Return 42" POC to GPUs to Quantum-Classical Hybrids</title>
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <!-- Google Fonts - Inter -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    
    <!-- Tailwind Configuration -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        primary: '#165DFF',
                        secondary: '#7B61FF',
                        dark: '#1E293B',
                        light: '#F8FAFC'
                    },
                    fontFamily: {
                        inter: ['Inter', 'sans-serif'],
                    },
                }
            }
        }
    </script>
    
    <style type="text/tailwindcss">
        .content-auto { content-visibility: auto; }
        .blog-content p { margin-bottom: 1.5rem; line-height: 1.7; }
        .blog-content h3 { font-size: 1.125rem; font-weight: 600; margin-top: 2rem; margin-bottom: 1rem; color: #1E293B; }
        .blog-content h4 { font-size: 1.0625rem; font-weight: 500; margin-top: 1.5rem; margin-bottom: 0.75rem; color: #1E293B; }
        .blog-content ul { list-style-type: disc; padding-left: 1.5rem; margin-bottom: 1.5rem; }
        .blog-content ol { list-style-type: decimal; padding-left: 1.5rem; margin-bottom: 1.5rem; }
        .blog-content code { background-color: #F3F4F6; padding: 0.125rem 0.375rem; border-radius: 0.25rem; font-size: 0.875rem; }
        .blog-content table { width: 100%; border-collapse: collapse; margin-bottom: 1.5rem; }
        .blog-content th, .blog-content td { border: 1px solid #E5E7EB; padding: 0.75rem; text-align: left; }
        .blog-content th { background-color: #F9FAFB; font-weight: 600; }
        .card-hover { transition: all 0.3s ease; }
        .card-hover:hover { box-shadow: 0 8px 20px rgba(0,0,0,0.08); transform: translateY(-4px); }
        .diagram { 
            font-family: monospace; 
            white-space: pre; 
            background-color: #f8fafc; 
            padding: 1rem; 
            border-radius: 0.5rem; 
            border: 1px solid #e2e8f0;
            margin: 1.5rem 0;
            overflow-x: auto;
        }
    </style>
</head>
<body class="font-inter bg-light text-dark antialiased">
    <!-- Navigation -->
    <header class="fixed w-full bg-white/90 backdrop-blur-sm shadow-sm z-50 transition-all duration-300">
        <div class="container mx-auto px-4 py-3 flex justify-between items-center">
            <a href="../index.html" class="flex items-center gap-2">
                <div class="w-10 h-10 rounded-lg bg-primary flex items-center justify-center">
                    <i class="fas fa-bolt text-white text-xl"></i>
                </div>
                <span class="text-xl font-bold text-primary">AIComputing101</span>
            </a>
            
            <!-- Desktop Navigation -->
            <nav class="hidden md:flex items-center gap-8">
                <a href="../index.html#about" class="font-medium hover:text-primary transition-colors">About</a>
                <a href="../index.html#vision" class="font-medium hover:text-primary transition-colors">Vision</a>
                <a href="../index.html#projects" class="font-medium hover:text-primary transition-colors">Projects</a>
                <a href="index.html" class="font-medium text-primary transition-colors">Blog</a>
                <a href="../index.html#about-us" class="font-medium hover:text-primary transition-colors">About Us</a>
                <a href="https://github.com/AIComputing101" target="_blank" class="flex items-center gap-1 px-4 py-2 bg-primary text-white rounded-lg hover:bg-primary/90 transition-colors">
                    <i class="fa fa-github"></i> GitHub
                </a>
            </nav>
            
            <!-- Mobile Menu Button -->
            <button id="mobile-menu-btn" class="md:hidden text-dark text-2xl">
                <i class="fa fa-bars"></i>
            </button>
        </div>
        
        <!-- Mobile Navigation -->
        <div id="mobile-menu" class="md:hidden hidden bg-white border-t">
            <div class="container mx-auto px-4 py-3 flex flex-col gap-4">
                <a href="../index.html#about" class="py-2 font-medium hover:text-primary transition-colors">About</a>
                <a href="../index.html#vision" class="py-2 font-medium hover:text-primary transition-colors">Vision</a>
                <a href="../index.html#projects" class="py-2 font-medium hover:text-primary transition-colors">Projects</a>
                <a href="index.html" class="py-2 font-medium text-primary transition-colors">Blog</a>
                <a href="../index.html#about-us" class="py-2 font-medium hover:text-primary transition-colors">About Us</a>
                <a href="https://github.com/AIComputing101" target="_blank" class="flex items-center gap-1 px-4 py-2 bg-primary text-white rounded-lg hover:bg-primary/90 transition-colors w-fit">
                    <i class="fa fa-github"></i> GitHub
                </a>
            </div>
        </div>
    </header>

    <main class="pt-24">
        <!-- Blog Post Header -->
        <section class="py-12 bg-gradient-to-br from-primary/5 to-secondary/5">
            <div class="container mx-auto px-4">
                <div class="max-w-3xl mx-auto">
                    <div class="flex items-center text-sm text-gray-500 mb-4 flex-wrap gap-2">
                        <span><i class="fa fa-calendar-o mr-1"></i> Oct 15, 2025</span>
                        <span class="mx-2">•</span>
                        <span><i class="fa fa-tag mr-1"></i> Compilers, GPU, Quantum</span>
                        <span class="mx-2">•</span>
                        <span><i class="fa fa-user-o mr-1"></i> Stephen Shao</span>
                    </div>
                    <h1 class="text-[clamp(1.8rem,3vw,2.5rem)] font-bold mb-6 leading-tight">The Evolution of Compilers: From "Return 42" POC to GPUs to Quantum-Classical Hybrids</h1>
                    <img src="https://picsum.photos/id/1/1200/600" alt="Compiler Evolution: CPU to GPU to Quantum" class="w-full h-64 md:h-80 object-cover rounded-xl shadow-sm mb-6">
                    <p class="text-gray-600 italic">Tracing how compilers grew from simple "return 42" translators to orchestrators of GPUs, ML-focused tools like Triton, and quantum-classical systems.</p>
                </div>
            </div>
        </section>

        <!-- Blog Post Content -->
        <section class="py-12 bg-white">
            <div class="container mx-auto px-4">
                <div class="max-w-3xl mx-auto blog-content">
                    <p>If you’ve ever built a minimal C compiler—one that only understands <code>return 42;</code>—you know the thrill of simplification: lexing text into tokens, parsing into an AST, generating x86_64 assembly, and watching an executable spit out a 42. But that tiny proof-of-concept (POC) is just the first step in a compiler’s evolutionary journey. As hardware grew from single-core CPUs to parallel GPUs and quantum processors, compilers evolved too—becoming bridges between high-level code, specialized hardware, and optimized libraries like BLAS or LAPACK.</p>

                    <h3>1. The Minimal Compiler: Foundations of "Lex → Parse → Codegen"</h3>
                    <p>Your POC compiler is the "hello world" of compiler design—and it’s where every key concept starts. Here’s its role:</p>

                    <div class="diagram">
Minimal Compiler Workflow
-------------------------
Source Code          →    Tokens            →    AST             →    IR             →    Machine Code
("return 42;")       →    (via Lexer)       →    (via Parser)    →    (IR Generator)  →    (Codegen)
                           [TOKEN_RETURN,     [ReturnStmt {      [IR_Return {       [movl $42, %eax;
                            TOKEN_INTEGER}]     value=42 }]        value=42 }]         ret]
                    </div>

                    <ul>
                        <li><strong>Core Stages</strong>: It turns raw text (<code>return 42;</code>) into executable code via four steps:
                            <ol class="list-decimal pl-6 mt-2 mb-2">
                                <li><strong>Lexer</strong>: Converts text to tokens (<code>TOKEN_RETURN</code>, <code>TOKEN_INTEGER(42)</code>).</li>
                                <li><strong>Parser</strong>: Validates syntax and builds an AST (a tree representing <code>ReturnStmt { value=42 }</code>).</li>
                                <li><strong>IR Generator</strong>: Creates platform-agnostic intermediate code (e.g., TACKY IR’s <code>IR_Return { value=42 }</code>).</li>
                                <li><strong>Codegen</strong>: Translates IR to x86_64 assembly (e.g., <code>movl $42, %eax; ret</code> for Linux).</li>
                            </ol>
                        </li>
                        <li><strong>Hardware Link</strong>: It only talks to single-core CPUs, using basic ABIs (like System V) to define how return values live in registers (<code>%eax</code>).</li>
                        <li><strong>Key Lesson</strong>: Abstraction layers matter. The IR (not the AST or assembly) lets you separate "what the code does" from "how the hardware runs it"—a principle that scales to all future compilers.</li>
                    </ul>

                    <h3>2. Modern C++ Compilers (GCC, Clang): Optimizing for General-Purpose CPUs</h3>
                    <p>As code grew more complex (think C++ templates, OOP, or multi-threading), compilers like GCC 14 or Clang 17 built on your POC’s core stages but added critical layers:</p>

                    <div class="diagram">
Modern C++ Compiler (GCC/Clang)
-------------------------------
Source Code      →    Preprocessor    →    Tokens/AST    →    Optimization Passes    →    Machine Code
(C++ Code)       →    (Macros, #include)    (Analysis)     →    (Constant folding,    →    (x86_64/ARM/
                                                              vectorization, etc.)     other ISAs)
                                                                      ↓
                                                               Link to Libraries
                                                              (OpenBLAS, etc.)
                    </div>

                    <ul>
                        <li><strong>Advanced Optimizations</strong>: They turn naive code into fast code:
                            <ul class="list-disc pl-6 mt-1 mb-1">
                                <li><strong>Constant Folding</strong>: Replaces <code>2+3</code> with <code>5</code> at compile time (your POC skipped this).</li>
                                <li><strong>Vectorization</strong>: Uses CPU SIMD registers (e.g., x86_64’s AVX-512) to run 8+ operations at once.</li>
                            </ul>
                        </li>
                        <li><strong>Library Integration</strong>: They link to optimized libraries (e.g., OpenBLAS for linear algebra) instead of reinventing wheels—saving months of work.</li>
                    </ul>

                    <h3>3. GPU Compilers: From Traditional (NVCC, ROCm HIP) to ML-Focused (Triton)</h3>
                    <p>GPUs revolutionized computing with thousands of cores, but they demanded compilers that could orchestrate parallelism. The first wave (NVIDIA’s NVCC, AMD’s ROCm HIP) focused on general-purpose GPU (GPGPU) tasks. A newer evolution—Triton—refined this for machine learning (ML), balancing programmability with ML-specific performance.</p>

                    <h4>NVIDIA’s NVCC: CUDA Ecosystem Specialization</h4>
                    <p>NVCC (NVIDIA CUDA Compiler) is tightly integrated with NVIDIA’s GPU hardware, prioritizing performance for NVIDIA’s SM (Streaming Multiprocessor) architecture:</p>

                    <div class="diagram">
NVIDIA NVCC Workflow
--------------------
CUDA Source Code
    │
    ├─→ Host Code ─→ LLVM IR ─→ x86_64/ARM Assembly
    │
    └─→ Device Code
         │
         ├─→ Parse CUDA Extensions (__global__, threadIdx)
         │
         ├─→ Generate PTX (Parallel Thread Execution)
         │
         ├─→ Optimize for NVIDIA SM Architecture
         │
         └─→ Compile to Cubin (GPU-specific binary)
              │
              └─→ Link with CUDA Libraries (cuBLAS, cuFFT)
                    </div>

                    <ul>
                        <li><strong>Heterogeneous Code Splitting</strong>:
                            <ul class="list-disc pl-6 mt-1 mb-1">
                                <li><strong>Host Code</strong>: CPU-bound logic (e.g., data setup) is compiled via LLVM IR to x86_64/ARM assembly, like modern C++ compilers.</li>
                                <li><strong>Device Code</strong>: GPU kernels (marked <code>__global__</code>) follow a GPU-specific pipeline:
                                    <ol class="list-decimal pl-6 mt-1 mb-1">
                                        <li>Parse CUDA extensions (e.g., <code>threadIdx.x</code> for thread IDs).</li>
                                        <li>Generate PTX (Parallel Thread Execution) — a GPU-agnostic IR for parallel threads.</li>
                                        <li>Optimize for NVIDIA SMs (e.g., shared memory banking to avoid conflicts).</li>
                                        <li>Compile to <strong>cubin</strong>: A binary format tailored to specific NVIDIA GPUs (e.g., sm_89 for Hopper).</li>
                                    </ol>
                                </li>
                            </ul>
                        </li>
                        <li><strong>Library Integration</strong>: Relies on CUDA-specific libraries like <code>cuBLAS</code> (GPU linear algebra), <code>cuFFT</code> (Fourier transforms), and <code>Thrust</code> (parallel algorithms).</li>
                    </ul>

                    <h4>AMD’s ROCm HIP: Portability-First Parallelism</h4>
                    <p>AMD’s ROCm (Radeon Open Compute) uses HIP (Heterogeneous-Compute Interface for Portability) to balance cross-vendor compatibility with AMD GPU performance. It’s designed to let developers write code once and run it on both AMD and NVIDIA GPUs:</p>

                    <div class="diagram">
AMD ROCm HIP Workflow
---------------------
HIP Source Code
    │
    ├─→ Host Code ─→ LLVM IR ─→ x86_64/ARM Assembly
    │
    └─→ Device Code
         │
         ├─→ Parse HIP Extensions (__global__, hipThreadIdx_x)
         │
         ├─→ Generate LLVM IR (with AMD GPU metadata)
         │
         ├─→ Optimize for AMD CDNA Architecture
         │
         └─→ Compile to Code Objects (AMD binary format)
              │
              └─→ Link with ROCm Libraries (hipBLAS, hipFFT)
                    </div>

                    <ul>
                        <li><strong>HIP: A Familiar, Portable Abstraction</strong>:
                            <p>HIP mimics CUDA syntax (e.g., <code>__global__</code> for kernels) but compiles to AMD’s hardware via <strong>HIP-Clang</strong>—a LLVM-based compiler that parses HIP code and splits it into host/device paths.</p>
                        </li>
                        <li><strong>Device Code Pipeline for AMD GPUs</strong>:
                            <ol class="list-decimal pl-6 mt-1 mb-1">
                                <li>Parse HIP extensions (e.g., <code>hipThreadIdx_x</code>—functionally identical to CUDA’s <code>threadIdx.x</code>).</li>
                                <li>Generate LLVM IR (shared with CPU compilers) augmented with AMD GPU metadata (e.g., for Infinity Fabric memory).</li>
                                <li>Optimize for AMD’s CDNA (Compute DNA) architecture (e.g., optimizing for MI300X’s stacked HBM3).</li>
                                <li>Compile to <strong>Code Objects</strong>: AMD’s binary format, compatible with all ROCm-enabled GPUs.</li>
                            </ol>
                        </li>
                        <li><strong>Portability Tools</strong>:
                            <ul class="list-disc pl-6 mt-1 mb-1">
                                <li><code>hipify-clang</code>: Converts CUDA code to HIP with minimal changes (e.g., <code>cudaMalloc</code> → <code>hipMalloc</code>).</li>
                                <li><code>hipBLAS</code>/<code>hipFFT</code>: API-compatible alternatives to NVIDIA’s <code>cuBLAS</code>/<code>cuFFT</code>, ensuring library portability.</li>
                            </ul>
                        </li>
                    </ul>

                    <h4>Triton Compiler: ML-Focused GPU Programming for Everyone</h4>
                    <p>Developed by OpenAI (now open-source), Triton represents a shift in GPU compiler design: it prioritizes <strong>ML workloads</strong> and <strong>programmer productivity</strong> without sacrificing performance. Unlike NVCC/HIP (which require low-level kernel writing), Triton lets developers write GPU-accelerated ML code in Python-like syntax.</p>

                    <div class="diagram">
Triton Compiler Workflow
------------------------
Python-like Triton Code
    │
    ├─→ Parse Triton Syntax
    │
    ├─→ Generate Triton IR (ML-optimized)
    │
    ├─→ ML-specific Optimizations
    │   (Autotuning, Tensor Coalescing, Operator Fusion)
    │
    └─→ Generate Target Code
         │
         ├─→ NVIDIA GPUs: PTX → Cubin
         │
         ├─→ AMD GPUs: LLVM IR → Code Objects
         │
         └─→ CPUs: LLVM IR → x86_64/ARM Assembly
              │
              └─→ Integration with PyTorch/TensorFlow
                    </div>

                    <ul>
                        <li><strong>Core Philosophy</strong>: "Write once, run fast on any GPU." Triton abstracts away GPU-specific details (threads, warps, shared memory) so ML researchers can focus on algorithms, not hardware.</li>
                        <li><strong>Compilation Pipeline</strong>:
                            <ol class="list-decimal pl-6 mt-1 mb-1">
                                <li><strong>High-Level Input</strong>: Triton kernels written in Python (e.g., a matrix multiplication function using Triton’s <code>tl.dot</code> for tensor operations).</li>
                                <li><strong>Frontend</strong>: Parses Python-like syntax into a <strong>Triton IR</strong>—an ML-optimized IR designed for tensor operations (e.g., handling batch dimensions, data types like FP16/FP8).</li>
                                <li><strong>Optimizations</strong>: ML-specific passes:
                                    <ul class="list-disc pl-6 mt-1 mb-1">
                                        <li><strong>Autotuning</strong>: Tests different kernel configurations (block sizes, memory layouts) to find the fastest one for the target GPU.</li>
                                        <li><strong>Tensor Coalescing</strong>: Groups memory accesses to reduce GPU memory latency (critical for ML’s large tensor operations).</li>
                                        <li><strong>Operator Fusion</strong>: Merges small tensor operations (e.g., add + relu) into a single kernel to avoid memory bottlenecks.</li>
                                    </ul>
                                </li>
                                <li><strong>Codegen</strong>: Translates optimized Triton IR to PTX (NVIDIA) or LLVM IR (AMD/CPU), then to hardware-specific binaries (cubin for NVIDIA, code objects for AMD).</li>
                            </ol>
                        </li>
                        <li><strong>Framework Integration</strong>: Tightly integrated with PyTorch and TensorFlow—developers can call Triton kernels directly from ML models (e.g., replacing PyTorch’s built-in <code>torch.matmul</code> with a custom Triton kernel).</li>
                        <li><strong>Why It’s an Evolution</strong>: Triton solves a key pain point of NVCC/HIP: ML researchers often aren’t GPU experts. It lets them write high-performance GPU code without learning low-level CUDA/HIP syntax—closing the gap between ML innovation and hardware performance.</li>
                    </ul>

                    <h4>GPU Compiler Comparison: NVCC vs. ROCm HIP vs. Triton</h4>
                    <table>
                        <thead>
                            <tr>
                                <th>Aspect</th>
                                <th>NVIDIA NVCC</th>
                                <th>AMD ROCm HIP</th>
                                <th>Triton Compiler</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Primary Focus</td>
                                <td>General GPGPU, NVIDIA-only</td>
                                <td>General GPGPU, cross-vendor</td>
                                <td>ML workloads (tensors), cross-vendor</td>
                            </tr>
                            <tr>
                                <td>Input Syntax</td>
                                <td>C/C++ with CUDA extensions</td>
                                <td>C/C++ with HIP extensions (CUDA-like)</td>
                                <td>Python-like (Triton dialect)</td>
                            </tr>
                            <tr>
                                <td>IR</td>
                                <td>PTX (GPU-agnostic)</td>
                                <td>LLVM IR (shared with CPU)</td>
                                <td>Triton IR (ML-optimized)</td>
                            </tr>
                            <tr>
                                <td>Key Strength</td>
                                <td>NVIDIA hardware optimization</td>
                                <td>Cross-vendor portability</td>
                                <td>ML productivity + autotuning</td>
                            </tr>
                            <tr>
                                <td>Target Users</td>
                                <td>GPGPU developers, NVIDIA-focused teams</td>
                                <td>Cross-vendor GPGPU developers</td>
                                <td>ML researchers, PyTorch/TensorFlow users</td>
                            </tr>
                        </tbody>
                    </table>

                    <h4>Best Practices for Modern GPU Compilers</h4>
                    <ol>
                        <li><strong>Choose the Right Tool for the Job</strong>: Use NVCC/HIP for general GPGPU tasks (e.g., scientific computing), Triton for ML workloads (e.g., custom tensor operations).</li>
                        <li><strong>Leverage Autotuning (Triton)</strong>: Let Triton’s autotuner optimize kernel configurations—manual tuning is rarely better for ML’s variable tensor sizes.</li>
                        <li><strong>Prioritize Portability</strong>: Use HIP or Triton if you need to support both NVIDIA and AMD GPUs (avoid vendor lock-in).</li>
                    </ol>

                    <h3>4. CUDA-Q: Quantum-Classical Hybrids</h3>
                    <p>The next frontier? Quantum computing. Compilers like NVIDIA’s CUDA-Q extend GPU compiler principles to quantum processors, linking classical CPU/GPU code with quantum circuits (e.g., <code>h(q)</code> for Hadamard gates) via a new abstraction layer: <strong>Quantum IR (QIR)</strong>.</p>

                    <div class="diagram">
CUDA-Q Workflow
---------------
Quantum-Classical Code
    │
    ├─→ Classical Code (CPU/GPU)
    │     │
    │     └─→ Compiled via NVCC/HIP/Triton
    │
    └─→ Quantum Code
          │
          ├─→ Parse Quantum Operations (h(q), cnot(q1,q2))
          │
          ├─→ Generate Quantum IR (QIR)
          │
          ├─→ Translate to OpenQASM
          │
          └─→ Execute on
               │
               ├─→ Quantum Hardware (e.g., NVIDIA DGX Quantum)
               │
               └─→ Quantum Simulators (via cuQuantum)
                    </div>

                    <p>CUDA-Q splits code into three paths: classical CPU/GPU logic (compiled via NVCC/HIP/Triton), quantum circuits (compiled to QIR → OpenQASM), and runtime integration with quantum hardware (e.g., NVIDIA DGX Quantum) or simulators (via <code>cuQuantum</code>).</p>

                    <h3>The Evolutionary Thread: Abstraction + Specialization</h3>
                    <p>Every step in compiler evolution boils down to two trends:</p>
                    <ol>
                        <li><strong>Abstraction Layers</strong>: From your POC’s TACKY IR to Triton’s ML-optimized IR and CUDA-Q’s QIR, compilers use IRs to keep code portable while adapting to hardware. Each new IR solves a specific problem (e.g., Triton IR for tensors, QIR for quantum gates).</li>
                        <li><strong>Domain Specialization</strong>: Compilers evolved from general-purpose tools (minimal C, GCC) to domain-specific ones:
                            <ul class="list-disc pl-6 mt-1 mb-1">
                                <li>NVCC/HIP: Specialized for GPGPU parallelism.</li>
                                <li>Triton: Specialized for ML’s tensor operations and researcher productivity.</li>
                                <li>CUDA-Q: Specialized for quantum-classical hybrid workflows.</li>
                            </ul>
                        </li>
                    </ol>
                    <p>Your minimal compiler taught you the basics. Modern compilers teach you the rest: a compiler’s true job is to make hard hardware problems easy to solve—without sacrificing speed. Whether you’re writing a "return 42" POC, a Triton kernel for ML, or a CUDA-Q quantum circuit, that’s the evolution that matters.</p>

                    <p class="font-medium mt-8">Final Tip: Start small (like your POC!) when learning new compilers. Master how NVCC/HIP splits host/device code, then try a simple Triton kernel (e.g., matrix multiplication) before jumping to CUDA-Q—each stage builds on the last. Happy compiling!</p>
                </div>
            </div>
        </section>

        <!-- Related Posts -->
        <section class="py-12 bg-gray-50">
            <div class="container mx-auto px-4">
                <div class="max-w-3xl mx-auto">
                    <h2 class="text-2xl font-semibold mb-8">Related Posts</h2>
                    <div class="grid md:grid-cols-2 gap-6">
                        <a href="compiler-optimizations-101.html" class="bg-white rounded-xl overflow-hidden shadow-sm card-hover">
                            <img src="https://picsum.photos/id/180/600/400" alt="Compiler Optimizations" class="w-full h-40 object-cover">
                            <div class="p-4">
                                <h3 class="font-semibold mb-1">10 Compiler Optimizations Every Dev Should Know</h3>
                                <p class="text-sm text-gray-500"><i class="fa fa-calendar-o mr-1"></i> Oct 8, 2025</p>
                            </div>
                        </a>
                        <a href="triton-kernels-for-pytorch.html" class="bg-white rounded-xl overflow-hidden shadow-sm card-hover">
                            <img src="https://picsum.photos/id/119/600/400" alt="Triton Kernels for PyTorch" class="w-full h-40 object-cover">
                            <div class="p-4">
                                <h3 class="font-semibold mb-1">Writing Fast ML Kernels with Triton & PyTorch</h3>
                                <p class="text-sm text-gray-500"><i class="fa fa-calendar-o mr-1"></i> Oct 12, 2025</p>
                            </div>
                        </a>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <!-- Footer -->
    <footer class="bg-dark text-white/80 py-12">
        <div class="container mx-auto px-4">
            <div class="grid md:grid-cols-4 gap-8 mb-10">
                <div>
                    <div class="flex items-center gap-2 mb-4">
                        <div class="w-10 h-10 rounded-lg bg-white flex items-center justify-center">
                            <i class="fas fa-bolt text-primary text-xl"></i>
                        </div>
                        <span class="text-xl font-bold text-white">AIComputing101</span>
                    </div>
                    <p class="mb-4">
                        Collaborative learning for AI and computational systems.
                    </p>
                    <div class="flex gap-4">
                        <a href="https://github.com/AIComputing101" target="_blank" class="text-white/60 hover:text-white transition-colors">
                            <i class="fa fa-github text-xl"></i>
                        </a>
                        <a href="#" class="text-white/60 hover:text-white transition-colors">
                            <i class="fa fa-twitter text-xl"></i>
                        </a>
                        <a href="#" class="text-white/60 hover:text-white transition-colors">
                            <i class="fa fa-youtube-play text-xl"></i>
                        </a>
                        <a href="#" class="text-white/60 hover:text-white transition-colors">
                            <i class="fa fa-discord text-xl"></i>
                        </a>
                    </div>
                </div>

                <div>
                    <h4 class="text-white font-semibold mb-4">Quick Links</h4>
                    <ul class="space-y-2">
                        <li><a href="../index.html#about" class="hover:text-white transition-colors">Our Mission</a></li>
                        <li><a href="../index.html#vision" class="hover:text-white transition-colors">Our Vision</a></li>
                        <li><a href="../index.html#projects" class="hover:text-white transition-colors">Projects</a></li>
                        <li><a href="index.html" class="hover:text-white transition-colors">Blog</a></li>
                        <li><a href="../index.html#about-us" class="hover:text-white transition-colors">About Us</a></li>
                    </ul>
                </div>

                <div>
                    <h4 class="text-white font-semibold mb-4">Resources</h4>
                    <ul class="space-y-2">
                        <li><a href="https://github.com/AIComputing101?tab=repositories" target="_blank" class="hover:text-white transition-colors">All Repositories</a></li>
                        <li><a href="#" class="hover:text-white transition-colors">Contribution Guide</a></li>
                        <li><a href="#" class="hover:text-white transition-colors">Learning Paths</a></li>
                        <li><a href="#" class="hover:text-white transition-colors">FAQ</a></li>
                        <li><a href="#" class="hover:text-white transition-colors">Contact</a></li>
                    </ul>
                </div>

                <div>
                    <h4 class="text-white font-semibold mb-4">Stay Updated</h4>
                    <p class="mb-4 text-sm">Subscribe to our newsletter for project updates and new resources.</p>
                    <form class="flex">
                        <input type="email" placeholder="Your email" class="px-4 py-2 rounded-l-lg w-full text-dark focus:outline-none text-sm">
                        <button type="submit" class="bg-primary px-4 py-2 rounded-r-lg hover:bg-primary/90 transition-colors">
                            <i class="fa fa-paper-plane"></i>
                        </button>
                    </form>
                </div>
            </div>

            <div class="border-t border-white/10 pt-8 text-center text-sm">
                <p>&copy; 2025 AIComputing101. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <!-- JavaScript -->
    <script>
        // Mobile menu toggle
        const mobileMenuBtn = document.getElementById('mobile-menu-btn');
        const mobileMenu = document.getElementById('mobile-menu');
        
        mobileMenuBtn.addEventListener('click', () => {
            mobileMenu.classList.toggle('hidden');
            const icon = mobileMenuBtn.querySelector('i');
            if (mobileMenu.classList.contains('hidden')) {
                icon.classList.remove('fa-times');
                icon.classList.add('fa-bars');
            } else {
                icon.classList.remove('fa-bars');
                icon.classList.add('fa-times');
            }
        });

        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                
                const targetId = this.getAttribute('href');
                if (targetId === '#') return;
                
                const targetElement = document.querySelector(targetId);
                if (targetElement) {
                    window.scrollTo({
                        top: targetElement.offsetTop - 80,
                        behavior: 'smooth'
                    });
                    
                    if (!mobileMenu.classList.contains('hidden')) {
                        mobileMenu.classList.add('hidden');
                        const icon = mobileMenuBtn.querySelector('i');
                        icon.classList.remove('fa-times');
                        icon.classList.add('fa-bars');
                    }
                }
            });
        });

        // Header scroll effect
        window.addEventListener('scroll', () => {
            const header = document.querySelector('header');
            if (window.scrollY > 50) {
                header.classList.add('py-2', 'shadow');
                header.classList.remove('py-3');
            } else {
                header.classList.add('py-3');
                header.classList.remove('py-2', 'shadow');
            }
        });
    </script>
</body>
</html>
